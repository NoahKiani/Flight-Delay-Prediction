{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b743d25",
   "metadata": {},
   "source": [
    "# Support Vector Regressor\n",
    "## Sample of 20,000\n",
    "\n",
    "This support vector regressor uses the stratified sample to reduce bias. For the features I chose all available features within the dataset. After evaluating the results I could possibly remove some of the features to make it not too complex for training. I also did not use a kernel function, as I already tried this in the first tryout, which didn't improve the accuracy. Together with Bas, I wrote the code, adapted from his github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd7fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\noahk\\anaconda3\\lib\\site-packages (2.6.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (0.14.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (1.5.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2022.7)\n",
      "Requirement already satisfied: six in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (23.0)\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8806d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\noahk\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\noahk\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f96a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6604.646078795233\n",
      "R^2 Score: -0.023246426366626327\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.read_csv('sample20000.csv')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "kolommen = ['ORIGIN', 'Airport_Region', 'Seizoen', 'ARR_DELAY', 'AIRLINE_CODE', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'CRS_ELAPSED_TIME', 'DISTANCE']\n",
    "df = df[kolommen]\n",
    "df = pd.get_dummies(df, columns=['ORIGIN', 'Seizoen', 'Airport_Region', 'AIRLINE_CODE'], drop_first=False)\n",
    "\n",
    "target = 'ARR_DELAY'\n",
    "X = df.drop(columns=[target])  \n",
    "y = df[target]\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)  \n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)) \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X_train, y_train.ravel()) \n",
    "\n",
    "\n",
    "y_pred_scaled = svr.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test_original, y_pred)\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a79846",
   "metadata": {},
   "source": [
    "# Results \n",
    "As can be seen, this model also does not have any predictive power. A mean absolute error of 81 is not good at all. If you put it in context, the average flight delay prediction is off by 81 minutes. This is not usable at all.<br>\n",
    "\n",
    "# What to do next?\n",
    "\n",
    "After talking to Bas, we came to the conclusion that we go back a step into data understanding. I'm going to perform some feature importance. First using a decision tree model with XGBoost and then with SHAP values. Even though this dataset has no predictive power, we can still say things about it. The decision tree model will tell me what features are used most in decisions. Those features are the most important ones in my dataset. By using SHAP values, I can see what the absolute effects  of the most important features are. For example, you can see how many delay minutes the model adds or subtracts when coming across a certain feature. Let's say that a certain airport adds an average of 50 minutes arrival delay, you can advice to avoid this airport. So, the model still can say usefull things.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
